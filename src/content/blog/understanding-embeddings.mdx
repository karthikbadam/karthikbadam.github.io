---
title: "Understanding Embeddings Through Epic Fantasy"
date: "2025-05-11"
author: "Karthik Badam"
description: "A journey through the world of embeddings, illustrated with epic fantasy literature and interactive visualizations"
---

import { ScatterPlot } from '../../components/ScatterPlot';

# Understanding Embeddings Through Epic Fantasy

In the world of Natural Language Processing (NLP), embeddings are like magical systems that transform words into numbers. Just as Allomancy in Mistborn or the One Power in The Wheel of Time can transmute and manipulate reality, embeddings convert text into numerical vectors that capture meaning and relationships.

## What are Embeddings?

Imagine you're in the Great Library of Tar Valon or the archives of the White Tower. Each book has its own unique characteristics: some are about the One Power, others about Allomancy, some focus on the history of Middle-earth, while others emphasize the Cosmere's magic systems. Embeddings work similarly by capturing these characteristics as numbers, allowing us to understand relationships between different pieces of text.

## Word2Vec: The Basic Magic System

Word2Vec is like the first magic system a novice learns. It creates embeddings by looking at the context of words. For example:

```python
# A simple Word2Vec example
from gensim.models import Word2Vec
sentences = [
    ['Kaladin', 'learns', 'to', 'use', 'Stormlight'],
    ['Gandalf', 'teaches', 'Frodo', 'about', 'the', 'Ring'],
    ['Vin', 'discovers', 'her', 'Mistborn', 'powers']
]
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)

# Let's see how similar characters are
print(model.wv.similarity('Kaladin', 'Frodo'))  # Similar heroes
print(model.wv.similarity('Kaladin', 'Sauron'))  # Hero vs Villain
```

If we show the Word2Vec numbers for some epic fantasy characters, they might look like this:

<ScatterPlot
  data={[
    { x: 0.88, y: 0.95, label: "Kaladin", category: "Hero" },
    { x: 0.82, y: 0.89, label: "Frodo", category: "Hero" },
    { x: 0.74, y: 0.80, label: "Gandalf", category: "Mentor" },
    { x: 0.70, y: 0.74, label: "Dalinar", category: "Mentor" },
    { x: 0.22, y: 0.38, label: "Sauron", category: "Villain" },
    { x: 0.30, y: 0.26, label: "Ruin", category: "Villain" }
  ]}
  title="Word2Vec: Character Relationships"
/>

Heroes might naturally cluster together, while mentors form their own distinct group, and villains are cast far into the shadows. This captures how Word2Vec represents semantic relationships.

## BERT: The Advanced Magic System

BERT (Bidirectional Encoder Representations from Transformers) is like a more powerful magic system that understands context from both directions. It's like having a Fullborn who can use both Allomancy and Feruchemy, or a channeler who can weave the One Power in complex patterns.

```python
from transformers import BertTokenizer, BertModel
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Let's encode some fantasy concepts with more complex relationships
texts = [
    "The Stormlight Archive tells the story of Kaladin, a former slave who becomes a Windrunner",
    "Mistborn follows Vin's journey as a Mistborn, learning to use Allomancy and Feruchemy",
    "The Wheel of Time weaves the tale of Rand al'Thor, the Dragon Reborn who can channel saidin",
    "The Lord of the Rings chronicles Frodo's quest to destroy the One Ring in Mount Doom",
    "The Way of Kings reveals Dalinar's struggle with his past and his bond with the Stormfather",
    "The Final Empire shows Kelsier's plan to overthrow the Lord Ruler using Allomancy"
]

# Get BERT embeddings with more detailed processing
embeddings = []
for text in texts:
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    outputs = model(**inputs)
    # Use CLS token embedding for better sentence-level representation
    embeddings.append(outputs.last_hidden_state[:, 0, :].detach().numpy())
```

Let's see how BERT might represent different fantasy concepts:

<ScatterPlot
  data={[
    { x: 0.92, y: 0.90, label: "Stormlight", category: "Magic System" },
    { x: 0.87, y: 0.84, label: "Allomancy", category: "Magic System" },
    { x: 0.83, y: 0.92, label: "Feruchemy", category: "Magic System" },
    { x: 0.90, y: 0.82, label: "The One Power", category: "Magic System" },
    { x: 0.60, y: 0.56, label: "Shardblade", category: "Artifact" },
    { x: 0.54, y: 0.46, label: "The One Ring", category: "Artifact" },
    { x: 0.62, y: 0.60, label: "Mistcloak", category: "Artifact" },
    { x: 0.50, y: 0.52, label: "Callandor", category: "Artifact" }
  ]}
  title="BERT: Concept Relationships"
/>

If we plot BERT's embeddings, we might see a clear separation between magic systems and artifacts, while maintaining relationships within each category. This is like how a skilled channeler can distinguish between different weaves of the One Power while understanding their underlying connections.

## Visualizing Embeddings: The Art of Pattern Weaving

Visualizing high-dimensional embeddings is like weaving the One Power into a spell or creating surges in the Cosmere - it transforms abstract concepts into tangible effects. There are many ways to do this. Let's explore two powerful techniques:

### t-SNE: The Delicate Weave

```python
from sklearn.manifold import TSNE
import numpy as np

# Create a more complex set of embeddings
# Each category has multiple subcategories and relationships
embeddings = np.random.rand(200, 300)  # 200 fantasy concepts in 300D space

# Apply t-SNE with more iterations for better convergence
tsne = TSNE(
    n_components=2,
    perplexity=30,
    n_iter=2000,
    learning_rate='auto',
    init='pca'
)
tsne_results = tsne.fit_transform(embeddings)
```

<ScatterPlot
  data={[
    { x: 0.93, y: 0.98, label: "The Stormlight Archive", category: "Cosmere" },
    { x: 0.89, y: 0.93, label: "Mistborn Era 1", category: "Cosmere" },
    { x: 0.85, y: 0.96, label: "Mistborn Era 2", category: "Cosmere" },
    { x: 0.97, y: 0.92, label: "Elantris", category: "Cosmere" },
    { x: 0.91, y: 0.90, label: "Warbreaker", category: "Cosmere" },
    { x: 0.66, y: 0.68, label: "The Lord of the Rings", category: "Middle-earth" },
    { x: 0.62, y: 0.73, label: "The Silmarillion", category: "Middle-earth" },
    { x: 0.70, y: 0.62, label: "The Hobbit", category: "Middle-earth" },
    { x: 0.60, y: 0.60, label: "Unfinished Tales", category: "Middle-earth" },
    { x: 0.22, y: 0.32, label: "The Eye of the World", category: "Wheel of Time" },
    { x: 0.36, y: 0.40, label: "The Great Hunt", category: "Wheel of Time" },
    { x: 0.28, y: 0.22, label: "The Dragon Reborn", category: "Wheel of Time" },
    { x: 0.40, y: 0.36, label: "The Shadow Rising", category: "Wheel of Time" },
    { x: 0.10, y: 0.88, label: "Stormlight", category: "Magic System" },
    { x: 0.18, y: 0.92, label: "Allomancy", category: "Magic System" },
    { x: 0.12, y: 0.80, label: "The One Power", category: "Magic System" },
    { x: 0.20, y: 0.84, label: "Feruchemy", category: "Magic System" }
  ]}
  title="t-SNE: Fantasy Series Clustering"
/>

Much like a delicate weave of the One Power, t-SNE preserves local relationships but can distort global structure. Notice how Cosmere books cluster tightly together, while maintaining some distance from Middle-earth works. This is particularly useful when we want to focus on the relationships between closely related concepts.

### UMAP: The Powerful Surge

```python
import umap

# Apply UMAP with more sophisticated parameters
reducer = umap.UMAP(
    n_components=2,
    n_neighbors=15,
    min_dist=0.1,
    metric='cosine',
    random_state=42
)
umap_results = reducer.fit_transform(embeddings)

# We can also use UMAP for higher dimensions
reducer_3d = umap.UMAP(n_components=3)
umap_3d_results = reducer_3d.fit_transform(embeddings)
```

<ScatterPlot
  data={[
    { x: 0.98, y: 0.99, label: "The Way of Kings", category: "Cosmere" },
    { x: 0.94, y: 0.92, label: "Words of Radiance", category: "Cosmere" },
    { x: 0.90, y: 0.97, label: "Oathbringer", category: "Cosmere" },
    { x: 0.96, y: 0.90, label: "Rhythm of War", category: "Cosmere" },
    { x: 0.92, y: 0.93, label: "The Final Empire", category: "Cosmere" },
    { x: 0.99, y: 0.91, label: "The Well of Ascension", category: "Cosmere" },
    { x: 0.88, y: 0.96, label: "The Hero of Ages", category: "Cosmere" },
    { x: 0.68, y: 0.72, label: "The Fellowship of the Ring", category: "Middle-earth" },
    { x: 0.72, y: 0.66, label: "The Two Towers", category: "Middle-earth" },
    { x: 0.64, y: 0.78, label: "The Return of the King", category: "Middle-earth" },
    { x: 0.70, y: 0.74, label: "The Silmarillion", category: "Middle-earth" },
    { x: 0.18, y: 0.38, label: "The Eye of the World", category: "Wheel of Time" },
    { x: 0.32, y: 0.29, label: "The Great Hunt", category: "Wheel of Time" },
    { x: 0.24, y: 0.33, label: "The Dragon Reborn", category: "Wheel of Time" },
    { x: 0.36, y: 0.27, label: "The Shadow Rising", category: "Wheel of Time" },
    { x: 0.20, y: 0.35, label: "The Fires of Heaven", category: "Wheel of Time" },
    { x: 0.08, y: 0.92, label: "Stormlight", category: "Magic System" },
    { x: 0.16, y: 0.99, label: "Allomancy", category: "Magic System" },
    { x: 0.12, y: 0.84, label: "Feruchemy", category: "Magic System" },
    { x: 0.18, y: 0.89, label: "The One Power", category: "Magic System" },
    { x: 0.10, y: 0.87, label: "Hemalurgy", category: "Magic System" }
  ]}
  title="UMAP: Fantasy Series Clustering"
/>

In contrast to t-SNE, UMAP is like a powerful surge of Stormlight. It can maintain both local and global structure more effectively. The clusters are more evenly distributed, and the relationships between different fantasy worlds are more clearly preserved. This makes it particularly useful when we need to understand both the fine details and the broader picture of our data.

## Conclusion

Embeddings are powerful tools that help us understand and work with text in ways that were once thought impossible. Just as epic fantasy literature transports us to worlds of complex magic systems and deep lore, embeddings transform our understanding of language into a mathematical space where we can discover new relationships and insights.

## Author's Note

The embeddings and visualizations shown in this article are fictional. They are meant to represent the idea in a simple way. I would personally love to revisit this article with real embeddings and live magical weavings (i.e., charts) in the future!